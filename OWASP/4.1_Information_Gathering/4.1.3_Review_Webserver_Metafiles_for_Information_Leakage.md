# Tóm tắt  
Phần này mô tả cách kiểm tra các tệp metadata khác nhau để phát hiện rò rỉ thông tin về đường dẫn hoặc chức năng của ứng dụng web. Hơn nữa, danh sách các thư mục cần tránh bởi các Spiders, Robots hoặc Crawlers cũng có thể được tạo như một phụ thuộc để lập bản đồ các đường dẫn thực thi qua ứng dụng. Các thông tin khác cũng có thể được thu thập để xác định bề mặt tấn công, chi tiết công nghệ hoặc sử dụng cho hoạt động tấn công xã hội (social engineering).

# Mục tiêu kiểm tra  
- Xác định các đường dẫn ẩn hoặc bị che giấu và chức năng thông qua phân tích các tệp metadata.  
- Trích xuất và lập bản đồ các thông tin khác có thể dẫn đến sự hiểu biết tốt hơn về hệ thống.  

# Cách kiểm tra  
Bất kỳ hành động nào dưới đây sử dụng `wget` cũng có thể được thực hiện với `curl`. Nhiều công cụ kiểm tra bảo mật ứng dụng động (DAST) như ZAP và Burp Suite bao gồm kiểm tra hoặc phân tích các tài nguyên này như một phần của chức năng spider/crawler của chúng. Chúng cũng có thể được xác định bằng cách sử dụng các Google Dorks hoặc tận dụng các tính năng tìm kiếm nâng cao như `inurl:`.

# Robots  
Các Web Spiders, Robots hoặc Crawlers truy xuất một trang web và sau đó lần theo các liên kết để truy xuất thêm nội dung web. Hành vi được chấp nhận của chúng được chỉ định bởi Robots Exclusion Protocol của tệp `robots.txt` trong thư mục gốc của web.

Ví dụ, đoạn đầu của tệp `robots.txt` từ Google (lấy mẫu ngày 5 tháng 5 năm 2020) như sau:

```
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
Disallow: /sdch
...
```

Chỉ thị User-Agent đề cập đến Spider/Robot/Crawler cụ thể. Ví dụ, `User-Agent: Googlebot` đề cập đến Spider của Google, trong khi `User-Agent: bingbot` đề cập đến Crawler của Microsoft. `User-Agent: *` áp dụng cho tất cả các Spider/Robot/Crawler.

Chỉ thị Disallow chỉ định các tài nguyên bị cấm bởi Spider/Robot/Crawler. Ví dụ:

```
Disallow: /search
Disallow: /sdch
```

Các Spider/Robot/Crawler có thể cố ý bỏ qua các chỉ thị `Disallow` được chỉ định trong tệp `robots.txt`, chẳng hạn như các mạng xã hội để đảm bảo rằng các liên kết được chia sẻ vẫn hợp lệ. Do đó, `robots.txt` không nên được coi là một cơ chế để thực thi các hạn chế về cách nội dung web được truy cập, lưu trữ hoặc tái bản bởi bên thứ ba.

Tệp `robots.txt` được truy xuất từ thư mục gốc của máy chủ web. Ví dụ, để truy xuất `robots.txt` từ www.google.com bằng `wget` hoặc `curl`:

```bash
$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
```

Phân tích robots.txt bằng Google Webmaster Tools  
Chủ sở hữu trang web có thể sử dụng chức năng "Analyze robots.txt" của Google như một phần của Google Webmaster Tools. Công cụ này hỗ trợ việc kiểm tra và quy trình thực hiện như sau:

1. Đăng nhập vào Google Webmaster Tools bằng tài khoản Google.
2. Trên bảng điều khiển, nhập URL của trang web cần phân tích.
3. Chọn phương pháp có sẵn và làm theo hướng dẫn trên màn hình.

# Thẻ META  
Thẻ `<META>` nằm trong phần HEAD của mỗi tài liệu HTML và nên nhất quán trên toàn trang web trong trường hợp Spider/Robot/Crawler bắt đầu từ một liên kết không phải từ gốc của trang web, tức là một liên kết sâu. Chỉ thị robots cũng có thể được chỉ định thông qua một thẻ META cụ thể.

## Robots META Tag  
Nếu không có mục `<META NAME="ROBOTS" ...>`, thì Robots Exclusion Protocol mặc định là `INDEX,FOLLOW`. Do đó, hai mục hợp lệ khác được định nghĩa bởi Robots Exclusion Protocol là các giá trị có tiền tố `NO...` tức là `NOINDEX` và `NOFOLLOW`.

Dựa trên các chỉ thị `Disallow` được liệt kê trong tệp `robots.txt` ở thư mục gốc, một tìm kiếm biểu thức chính quy cho `<META NAME="ROBOTS"` trên mỗi trang web sẽ được thực hiện và kết quả được so sánh với tệp `robots.txt` ở thư mục gốc.

## Các thẻ META thông tin khác  
Các tổ chức thường nhúng các thẻ META thông tin trong nội dung web để hỗ trợ cho các công nghệ khác nhau như trình đọc màn hình, xem trước mạng xã hội, lập chỉ mục công cụ tìm kiếm, v.v. Những meta-thông tin này có thể có giá trị đối với người kiểm tra trong việc xác định các công nghệ được sử dụng, các đường dẫn hoặc chức năng bổ sung để khám phá và kiểm tra.

Ví dụ: các meta-thông tin sau được truy xuất từ www.whitehouse.gov qua View Page Source vào ngày 5 tháng 5 năm 2020:

```html
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The White House" />
<meta property="og:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta property="og:url" content="https://www.whitehouse.gov/" />
<meta property="og:site_name" content="The White House" />
<meta property="fb:app_id" content="1790466490985150" />
<meta property="og:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
```

# Sitemaps  
Sitemap là một tệp nơi nhà phát triển hoặc tổ chức có thể cung cấp thông tin về các trang, video và tệp khác mà trang hoặc ứng dụng cung cấp, cùng với mối quan hệ giữa chúng. Công cụ tìm kiếm có thể sử dụng tệp này để khám phá trang web của bạn một cách thông minh hơn. Người kiểm tra có thể sử dụng tệp sitemap.xml để tìm hiểu thêm về trang web hoặc ứng dụng và khám phá nó hoàn chỉnh hơn.

Ví dụ sau đây là một phần của sitemap chính của Google được truy xuất ngày 5 tháng 5 năm 2020:

```bash
$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> "sitemap.xml" [1]

<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.google.com/schemas/sitemap/0.84">
  <sitemap>
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
```

# Security TXT  
`security.txt` là một tiêu chuẩn đề xuất cho phép các trang web xác định các chính sách bảo mật và chi tiết liên hệ. Các lý do để sử dụng trong kiểm tra bảo mật bao gồm:

- Xác định thêm các đường dẫn hoặc tài nguyên.
- Thu thập thông tin tình báo nguồn mở.
- Tìm kiếm thông tin về Bug Bounties.
- Tấn công xã hội (social engineering).

Tệp này có thể có mặt trong thư mục gốc của máy chủ web hoặc trong thư mục `.well-known/`. Ví dụ:

- https://example.com/security.txt
- https://example.com/.well-known/security.txt

# Humans TXT  
`humans.txt` là một sáng kiến để biết về những người đứng sau trang web. Nó thường chứa thông tin về những người đã đóng góp vào việc xây dựng trang web. 

# Các nguồn thông tin khác trong thư mục `.well-known`  
Có các RFCs và bản nháp Internet khác đề xuất các cách sử dụng chuẩn hóa cho các tệp trong thư mục `.well-known/`.

Người kiểm tra có thể dễ dàng xem xét các RFCs/bản nháp này và tạo danh sách để cung cấp cho crawler hoặc fuzzer nhằm xác minh sự tồn tại hoặc nội dung của các tệp đó.

# Công cụ

- Browser (View Source hoặc Dev Tools functionality): Trình duyệt có thể được sử dụng để xem nguồn mã HTML của một trang và kiểm tra các thẻ meta, robots hoặc các thông tin liên quan khác.
  
- curl: Một công cụ dòng lệnh để chuyển dữ liệu với các cú pháp URL, có thể sử dụng để tải xuống các tệp như `robots.txt`, `sitemap.xml`, hoặc các tệp `.well-known/`.
  
- wget: Tương tự như `curl`, nhưng có thêm các tùy chọn để tải về nhiều tệp hoặc tải nội dung web hoàn chỉnh.
  
- Burp Suite: Một trong những công cụ hàng đầu để kiểm tra bảo mật ứng dụng web, bao gồm khả năng spider, kiểm tra tệp `robots.txt`, phân tích sitemap, và phát hiện các điểm yếu khác.
  
- ZAP (OWASP Zed Attack Proxy): Một công cụ kiểm tra bảo mật nguồn mở và miễn phí, cung cấp các tính năng tương tự như Burp Suite nhưng phù hợp với các nhu cầu kiểm tra bảo mật cơ bản và trung bình.
